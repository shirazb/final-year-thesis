\section{Automatic Differentiation}
Evidently, training a neural network involves taking derivates of a computation (applying the network to the batch of data to get the loss).
How do we actually evaluate derivates on a computer?
There are three main approaches: numerical differentiation, symbolic differentiation, and automatic differentiation (AD).
In this section, I will show how automatic differentiation is used in machine learning.
Baydin et al. \cite{Baydin2015} have an excellent survey paper on the intersection of AD and ML;
covering the pitfalls of the other techniques, both forward- and reverse-mode AD, how to implement AD, and the adoption of AD in ML libraries.
I will merely give a brief motivation of AD over the other techniques and show how backpropagation is an instance of \textit{reverse-mode} AD.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overview}
Numerical differentiation refers to finite-difference methods, which evaluate the original function at two sample points and plug this into the limit definition of a derivative.
This is highly prone to round-off and truncation errors \cite{Jerrell1997}.
It also requires sampling the function for each partial derivative required, so does not scale to the millions of parameters in a neural network.

Symbolic differentiation refers to the mechanistic application of symbolic differentiation rules to a symbolic expression tree.
This is slow and suffers from `expression swell' \cite{Juedes1991} - the rote application of rules causing the size of the derivative expression to become large and complex.

Moreover, both these techniques require the model to be defined in a closed-form expression.
This is extremely difficult for neural networks, especially in the case where the model contains control flow, common in RNNs for example.

Automatic differentiation, also known as algorithmic differentiation or just autodiff,
is a family of techniques that gives evaulted numerical derivatives .

% Clearly, backprop algo requires evaluating derivatives
% How to take derivatives of a programmed computation?
% Numerical finite-difference bad
% Symbolic bad
% AD is a powerful family of techniques that gives evaluated numerical derivates, not a symbolic expression, by using the pre-programmed symbolic derivates of the operators applied and the chain rule to propagate derivates between them.
% As the user code applies operators, the AD library will augment the values to store derivative values too, and the operators to calculate their derivatives and propagate them to the next operators.
% That is, both the forward and backward functions of operators are implemented by the AD library programmer.
% The user uses the forward functions as usual, and the derivates will automatically be evaluated.
% If user applies y = f(g(x)), chain rule is used to propagate derivatives between the operator-wise backward functions, so the derivative of the entire computation can be computed.

% accurate evaluation at machine precision, small constant factor overhead and idelal asymptotic efficiency
% No need for the user to derive closed-form symbolic expressions or for the library to symbolically differentiate them, both arduous tasks.
% Any computation can be run imperatively, using native control flow etc, the backward operations will automatically occur

% AD should sound a lot like what happens in backprop.
% We run a series of forward operations and need to take the derivate of the entire computation.
% We know the derivates of each operator, so we take the overall derivate by using the chain rule.
% => backprop is specific case of AD

% Backprop is best suited to rv-mode AD, as opposed to fwd-mode.
% Though we will not cover, suffice to say forward mode, in one pass, computes any number outputs wrt to one input
% rv-mode can handle one output wrt to any number of inputs.
% Backprop algo has one output (loss) and possibly millions of inputs (params)

% rv autodiff.