In this section, I present the relevant background material and motivate my thesis. First, Deep Learning and backpropagation are introduced insofar as deriving its computational graph, which the memory optimisations are based on. I then show how backpropagation is a specific instance of \textit{reverse-mode automatic differentiation}. I introduce PyTorch, the ML framework I will use, and show how its Autograd module performs reverse-mode automatic differentiation. This will be relevant to how my implementation works. Finally, I introduce the checkpointing memory optimisation and appraise the current approaches, motivating my contributions.
