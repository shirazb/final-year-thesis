In this chapter, I will give a qualitative evaluation of my work and suggest directions for further work.
I will describe the limitations of my technique and, where possible, suggest ideas for remedying them.
I will assess the role of checkpointing in relation to other optimisation techniques and discuss how they can be combined based on their various merits and drawbacks.

\begin{enumerate}
    \item Limitation to sequences
    \begin{itemize}
        \item only sequences. actually only trees.
        \item But NNs graphs
        \item Must perform treedecomp to trees, but unknown treewidth even though probably small
        \item divide-and-conquer checkpointing generalises REVOLVE to trees, but is arbitrary C-M
        \item Two new papers: `graph theoretic framework' and `re-forward' generalise checkpointing to graphs. How well? Require user help? Arbitrary costs? Multiple Recomps? C-M?
    \end{itemize}
    \item Intractability on M and unneeded perfect optimality in real world: solve with RL techniques
    \begin{itemize}
        \item Real world M like 12GB (12e9) is way too big because algo iterates over every single m.
        \item Have to do this because M is a state of our `MDP', but now it is like a very large \textit{continuous} state.
        \item Takes way too long to converge! We should use a constrained MDP on m instead.
        \item I think, fundamentally, so much of effort in getting new algorithm to work is because of this continuous m state.
        \item Also, bucketing, after a certain extent, is a crude solution. causes internal fragmentation in buckets.
        \item Algorithm takes so much effort to be \textit{perfect}, only to be ruined by bucketing
        \item Don't actually need to be anywhere near that perfect anyway, and don't have the information to do so either
        \item Exact costs and especially memory budget very hard to profile
        \item So, instead have...
        \item Does not give perfect answer but will converge to `good enough' far faster than my algo will reach perfect and terminate.
    \end{itemize}
    \item Implementation in PyTorch
    \begin{itemize}
        \item Not good API at all. Needs to be an Autograd function, not some helper that does both forwards and backwards manually.
        \item Forces it to be a separate helper you call in train loop, rather than a function you naturally use and backward is automatically set up - completely against the spirit of PyTorch!
        \item Likely requires considerable fudging of existing user models where only a sub-model is sequential to make compatabile with this API. This case describes just about every case.
        \item PyTorch graph really does not correspond to our theoretic graph...
    \end{itemize}
    \item Relation to other techniques, esp. swapping, defrag, conv alg 
    \begin{itemize}
        \item C-M curves show as memory really gets tight, compute cost gets really bad
        \item This is where swapping is much better, can often get a theoretical overhead of 0
        \item But really hard to do - although maybe these papers like PipeDream can suggest how to do the required pipelining
        \item Checkpointing still useful as often faster to do than swap. Or really, more about freeing up PCIe bandwidth, allowing all future swaps to be done earlier, possibly removing a bottleneck. However, swapping something makes it not eligible for checkpointing.
        \item Can actually perform my DP checkpointing on a sequence with an imposed swapping schedule and find the optimal combined schedule.
        \item However, did not have time to implement and later found that already somewhat suggested.
        Swapping can be considered as a generalisation of checkpointing to multiple layers of memory - swapping is simply checkpointing to the 2nd layer and incurs a transfer cost. Work on this already...
        \item Really really intractable though, especially cos swapping. Becomes something like a Resource Constrained Scheduling Problem.
        \item Sync or async swapping? uniform costs? optimal compute given M?
        \item Singapore paper seems most promising, explain+evaluate...
        \item Next, consider choice of conv alg.
        \item Just like in DBs, search over logical ops and their physical implementations.
        \item vDNN figure shows workspace memory is quite significant, and that optimising for this has a significant effect.
        \item Can implement in my checkpointing framework - extend actions to be over all possible \(k, \mathrm{alg}\). Going to be even slower!
        \item Finally, look at reducing memory pool fragmentation.
        \item My alg assumes no fragmentation, we can always perfectly allocate \(x\) amount of memory without worrying about whether \(x\) consecutive bytes exist in the pool of size \(M\).
        \item Online defrag methods exist, but offline bin-packing heuristic algo from Singapore paper seems really promising
        \item As checkpointing done offline, should be theoretically possible to plan memory placement in light of checkpointing.
        \item \textbf{Conclusion:} Though limited for really tight M, checkpointing certainly has its place. Just look at significantly increased number of papers in last couple years. Is quite suited to combination with choice of conv alg, as can naturally extend DP framework. I think swapping will be the most heavily used if can be done right, as it gives near 0 overhead, and checkpointing will supplement it to reduce the overhead even further where possible. But difficulty of overhauling the memory manager of framework...
    \end{itemize}
    \item Really future work
    \begin{itemize}
        \item Really need to reconcile knowledge with AD, DBs, compilers, general DAGs
    \end{itemize}
\end{enumerate}